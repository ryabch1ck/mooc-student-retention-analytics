{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Предобработка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_filter(data, days=2):\n",
    "    \n",
    "    \"\"\"Фильтрация данных до порогового значения\"\"\"\n",
    "    \n",
    "    # создаем таблицу с первым и последним действием юзера\n",
    "    min_max_user_time = data.groupby('user_id').agg({'timestamp': 'min'}) \\\n",
    "                            .rename(columns={'timestamp': 'min_timestamp'}) \\\n",
    "                            .reset_index()\n",
    "    \n",
    "    data_time_filtered = pd.merge(data, min_max_user_time, on='user_id', how='outer')\n",
    "    \n",
    "    # отбираем те записи, которые не позднее двух дней с начала учебы\n",
    "    learning_time_threshold = days * 24 * 60 * 60\n",
    "    data_time_filtered = data_time_filtered.query(\"timestamp <= min_timestamp + @learning_time_threshold\")\n",
    "    \n",
    "    assert data_time_filtered.user_id.nunique() == data.user_id.nunique()\n",
    "    \n",
    "    return data_time_filtered.drop(['min_timestamp'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_features(events_data, submission_data):\n",
    "    \n",
    "    \"\"\"Создание датасета с базовыми фичами: действия юзера \n",
    "    и правильные\\неправильные ответы\"\"\"\n",
    "    \n",
    "    # построим таблицу со всеми действиями юзеров\n",
    "    users_events_data = pd.pivot_table(data=events_data, values='step_id',\n",
    "                                   index='user_id', columns='action',\n",
    "                                   aggfunc='count', fill_value=0) \\\n",
    "                                   .reset_index() \\\n",
    "                                   .rename_axis('', axis=1)\n",
    "    \n",
    "    # таблица с колво правильных и неправильных попыток\n",
    "    users_scores = pd.pivot_table(data=submission_data, \n",
    "                              values='step_id',\n",
    "                              index='user_id',\n",
    "                              columns='submission_status',\n",
    "                              aggfunc='count',\n",
    "                              fill_value=0).reset_index() \\\n",
    "                              .rename_axis('', axis=1)\n",
    "    \n",
    "    # соединяем в один датасет\n",
    "    users_data = pd.merge(users_scores, users_events_data, on='user_id', how='outer').fillna(0)\n",
    "    \n",
    "    assert users_data.user_id.nunique() == events_data.user_id.nunique()\n",
    "    \n",
    "    return users_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target(submission_data, threshold=40):\n",
    "    \n",
    "    \"\"\"Вычисление целевой переменной. Если юзер сделал 40 практический заданий,\n",
    "    то будем считать, что он пройдет курс до конца\"\"\"\n",
    "    \n",
    "    # считаем колво решенных заданий у каждого пользователя\n",
    "    users_count_correct = submission_data[submission_data.submission_status == 'correct'] \\\n",
    "                .groupby('user_id').agg({'step_id': 'count'}) \\\n",
    "                .reset_index().rename(columns={'step_id': 'corrects'})\n",
    "    \n",
    "    # если юзер выполнил нужное колво заданий, то он пройдет курс до конца\n",
    "    users_count_correct['passed_course'] = (users_count_correct.corrects >= threshold).astype('int')\n",
    "    \n",
    "    return users_count_correct.drop(['corrects'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_features(events_data):\n",
    "    \n",
    "    \"\"\"Создание временных фичей\"\"\"\n",
    "    \n",
    "    # добавление колонок с датами\n",
    "    events_data['date'] = pd.to_datetime(events_data['timestamp'], unit='s')\n",
    "    events_data['day'] = events_data['date'].dt.date\n",
    "    \n",
    "    # создаем таблицу с первым\\последним действием юзера и колвом уникальных дней, проведенных на курсе\n",
    "    users_time_feature = events_data.groupby('user_id').agg({'timestamp': ['min', 'max'], 'day': 'nunique'}) \\\n",
    "                        .droplevel(level=0, axis=1) \\\n",
    "                        .rename(columns={'nunique': 'days'}) \\\n",
    "                        .reset_index()\n",
    "    \n",
    "   \n",
    "    \n",
    "    return users_time_feature.drop(['max', 'min'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def steps_tried(submission_data):\n",
    "    \n",
    "    \"\"\"Создание фичи с колвом уникальных шагов, которые пользователь пытался выполнить\"\"\"\n",
    "    \n",
    "    # сколько степов юзер попытался сделать\n",
    "    steps_tried = submission_data.groupby('user_id').agg({'step_id' : 'nunique' }).reset_index() \\\n",
    "                                        .rename(columns={'step_id': 'steps_tried'})\n",
    "    \n",
    "    return steps_tried"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_ratio(data):\n",
    "    \n",
    "    \"\"\"Создание фичи с долей правильных ответов\"\"\"\n",
    "    \n",
    "    data['correct_ratio'] = (data.correct / (data.correct + data.wrong)).fillna(0)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Создание датасетов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df(events_data, submission_data):\n",
    "    \n",
    "    \"\"\"функция для формирования X датасета и y с целевыми переменными\"\"\"\n",
    "    \n",
    "    # фильтруем данные по дням от начала учебы\n",
    "    events_2days = time_filter(events_data)\n",
    "    submissions_2days = time_filter(submission_data)\n",
    "    \n",
    "    # создаем таблицу с базовыми фичами\n",
    "    users_data = base_features(events_2days, submissions_2days)\n",
    "    \n",
    "    # создаем целевую переменную\n",
    "    users_target_feature = target(submission_data, threshold=40)\n",
    "    \n",
    "    # создаем таблицу с временными фичами\n",
    "    users_time_feature = time_features(events_2days)\n",
    "    \n",
    "    # создаем фичи с попытками степов и долей правильных ответов\n",
    "    users_steps_tried = steps_tried(submissions_2days)\n",
    "    users_data = correct_ratio(users_data)\n",
    "    \n",
    "    # соединяем шаги\n",
    "    first_merge = users_data.merge(users_steps_tried, how='outer').fillna(0)\n",
    "    \n",
    "    # соединяем фичи со временем\n",
    "    second_merge = first_merge.merge(users_time_feature, how='outer')\n",
    "    \n",
    "    # присоединяем целевую переменную\n",
    "    third_merge = second_merge.merge(users_target_feature, how='outer').fillna(0)\n",
    "    \n",
    "    # отделяем целевую переменную и удаляем ее из основного датасета\n",
    "    y = third_merge['passed_course'].map(int)\n",
    "    X = third_merge.drop(['passed_course'], axis=1)\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_df(events_data, submission_data):\n",
    "    \n",
    "    \"\"\"функция для формирования test датасета без целевой переменной\"\"\"\n",
    "    \n",
    "    # фильтруем данные по дням от начала учебы\n",
    "    events_2days = time_filter(events_data)\n",
    "    submissions_2days = time_filter(submission_data)\n",
    "    \n",
    "    # создаем таблицу с базовыми фичами\n",
    "    users_data = base_features(events_2days, submissions_2days)\n",
    "    \n",
    "    \n",
    "    # создаем таблицу с временными фичами\n",
    "    users_time_feature = time_features(events_2days)\n",
    "    \n",
    "    # создаем фичи с попытками степов и долей правильных ответов\n",
    "    users_steps_tried = steps_tried(submissions_2days)\n",
    "    users_data = correct_ratio(users_data)\n",
    "    \n",
    "    # соединяем шаги\n",
    "    first_merge = users_data.merge(users_steps_tried, how='outer').fillna(0)\n",
    "    \n",
    "    # соединяем фичи со временем\n",
    "    X = first_merge.merge(users_time_feature, how='outer')\n",
    "       \n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# загрузка тренировочного датасета\n",
    "events_data_train = pd.read_csv('D:\\Загрузки/event_data_train.csv')\n",
    "submission_data_train = pd.read_csv('D:\\Загрузки/submissions_data_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создание тренировочного датасета с нужными фичами и целевой переменной\n",
    "X_train, y = create_df(events_data_train, submission_data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# загрузка тестового датасета\n",
    "events_data_test = pd.read_csv('D:\\Загрузки/events_data_test.csv')\n",
    "submission_data_test = pd.read_csv('D:\\Загрузки/submission_data_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создание тестового датасета\n",
    "X_test = create_test_df(events_data_test, submission_data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_final_improved(train_data, y, test_data, size=0.20):\n",
    "    \n",
    "    \"\"\"Улучшенный пайплайн с предобработкой данных\"\"\"\n",
    "    \n",
    "    test_data = test_data.sort_values('user_id')\n",
    "    \n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(train_data, y, test_size=size, random_state=42)\n",
    "\n",
    "    numeric_features = [col for col in X_train.columns if col != 'user_id']\n",
    "    \n",
    "    # Создаем препроцессор\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', StandardScaler(), numeric_features)  # Масштабируем только числовые фичи\n",
    "        ]\n",
    "        # remainder по умолчанию 'drop' - остальные колонки (user_id) удаляются\n",
    "    )\n",
    "    \n",
    "    # Создаем комплексный пайплайн\n",
    "    pipe = make_pipeline(\n",
    "        preprocessor, # Масштабирование признаков\n",
    "        RandomForestClassifier(\n",
    "            max_depth=6, \n",
    "            n_estimators=28,  \n",
    "            random_state=42,\n",
    "            min_samples_split=12\n",
    "        )\n",
    "    )\n",
    "  \n",
    "    # Обучаем пайплайн\n",
    "    pipe.fit(X_train, y_train)\n",
    "    \n",
    "    # Предсказания на валидации\n",
    "    ypred_prob = pipe.predict_proba(X_valid)\n",
    "    \n",
    "    roc_score = roc_auc_score(y_valid, ypred_prob[:, 1])\n",
    "    score = pipe.score(X_valid, y_valid)\n",
    "    print(f\"Правильность на валид наборе: {score:.3f}\")\n",
    "    print(f\"Roc_auc_score на валид наборе: {roc_score:.5f}\")\n",
    "    \n",
    "    # Предсказания на тестовых данных\n",
    "    ypred_prob_final = pipe.predict_proba(test_data)\n",
    "    result = test_data['user_id'].to_frame()\n",
    "    result['is_gone'] = ypred_prob_final[:, 1]\n",
    "    result[['user_id', 'is_gone']].to_csv(f'my_predict_improved_{roc_score:.5f}.csv', index=False)\n",
    "    print(f'Результаты записаны в файл my_predict_improved_{roc_score:.5f}.csv')\n",
    "\n",
    "    return pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Дополнительная функция для подбора параметров пайплайна\n",
    "def random_with_grid_improved(train_data, y, size=0.20):\n",
    "    \n",
    "    \"\"\"Поиск наилучших параметров для улучшенного пайплайна\"\"\"\n",
    "    \n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(train_data, y, test_size=size, random_state=42)\n",
    "    \n",
    "    # Параметры для GridSearch\n",
    "    param_grid = {\n",
    "        'randomforestclassifier__n_estimators': range(25,33),\n",
    "        'randomforestclassifier__max_depth': range(5,8),\n",
    "        'randomforestclassifier__min_samples_split': range(10,15)\n",
    "    }\n",
    "    \n",
    "    # Создаем пайплайн\n",
    "    pipe = make_pipeline(\n",
    "        SimpleImputer(strategy='median'),\n",
    "        StandardScaler(),\n",
    "        SelectKBest(f_classif),\n",
    "        RandomForestClassifier(random_state=42, class_weight='balanced')\n",
    "    )\n",
    "    \n",
    "    # Поиск по сетке параметров\n",
    "    grid = GridSearchCV(pipe, param_grid=param_grid, cv=5, n_jobs=-1, scoring='roc_auc')\n",
    "    grid.fit(X_train, y_train)\n",
    "    \n",
    "    print(f\"Наилучшие параметры: {grid.best_params_}\")\n",
    "    print(f\"Лучший ROC-AUC на кросс-валидации: {grid.best_score_:.5f}\")\n",
    "    \n",
    "    # Оценка на валидации\n",
    "    ypred_prob = grid.predict_proba(X_valid)\n",
    "    roc_score = roc_auc_score(y_valid, ypred_prob[:, 1])\n",
    "    print(f\"ROC-AUC на валидации: {roc_score:.5f}\")\n",
    "    \n",
    "    return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== УЛУЧШЕННЫЙ ПАЙПЛАЙН ===\n",
      "Правильность на валид наборе: 0.904\n",
      "Roc_auc_score на валид наборе: 0.87759\n",
      "Результаты записаны в файл my_predict_improved_0.87759.csv\n"
     ]
    }
   ],
   "source": [
    "# Запуск улучшенной версии\n",
    "print(\"=== УЛУЧШЕННЫЙ ПАЙПЛАЙН ===\")\n",
    "improved_model = random_final_improved(X_train, y, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Опционально: поиск лучших параметров (может занять время)\n",
    "#print(\"=== ПОДБОР ПАРАМЕТРОВ ===\")\n",
    "#best_grid = random_with_grid_improved(X_train, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
